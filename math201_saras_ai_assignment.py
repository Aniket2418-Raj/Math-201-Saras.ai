# -*- coding: utf-8 -*-
"""Math201-Saras Ai Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XInipdpeoXuI6q-aXsFu_38w-o1ZV6Pi

**Week 1**

**Load FashionMNIST dataset using a deep learning library**
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.datasets import fashion_mnist

"""**Load the FashionMNIST Dataset**"""

# Load dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Print shapes
print("Training data shape:", X_train.shape)
print("Training labels shape:", y_train.shape)
print("Test data shape:", X_test.shape)
print("Test labels shape:", y_test.shape)

"""**Define Class Labels**"""

class_names = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

"""**Data Preprocessing**"""

# Normalize pixel values (0–255 → 0–1)
X_train = X_train / 255.0
X_test = X_test / 255.0

# Check value range
print("Min pixel value:", X_train.min())
print("Max pixel value:", X_train.max())

"""**Visualize Sample Images**"""

plt.figure(figsize=(10, 5))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[i], cmap="gray")
    plt.title(class_names[y_train[i]])
    plt.axis("off")

plt.tight_layout()
plt.show()

"""**Category Distribution**"""

plt.figure(figsize=(10, 4))
sns.countplot(x=y_train)
plt.xticks(ticks=range(10), labels=class_names, rotation=45)
plt.title("Distribution of Fashion Categories in Training Set")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

"""**Images per Category**"""

plt.figure(figsize=(10, 6))

for i in range(10):
    idx = np.where(y_train == i)[0][0]
    plt.subplot(2, 5, i + 1)
    plt.imshow(X_train[idx], cmap="gray")
    plt.title(class_names[i])
    plt.axis("off")

plt.tight_layout()
plt.show()

"""**Basic Dataset Insights**"""

print("Number of training samples:", X_train.shape[0])
print("Number of test samples:", X_test.shape[0])
print("Image dimensions:", X_train.shape[1], "x", X_train.shape[2])
print("Number of classes:", len(class_names))

"""**EDA Summary**

### Exploratory Data Analysis Summary

- The FashionMNIST dataset contains 60,000 training images and 10,000 test images.
- Each image is a 28×28 grayscale representation of a fashion item.
- Pixel values were normalized to the range [0, 1] for stable neural network training.
- The dataset is well-balanced across all 10 categories.
- Visual inspection shows clear structural differences between footwear, clothing, and accessories.
- Some classes (e.g., Shirt vs T-shirt/top) appear visually similar, indicating a potential challenge for classification models.

**Week 2**

***Neural Network Building and Training***

Import Required Libraries
"""

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

"""Load and Preprocess Data"""

# Load dataset
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()

# Normalize
X_train = X_train / 255.0
X_test = X_test / 255.0

# One-hot encode labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

print("Training shape:", X_train.shape)
print("Test shape:", X_test.shape)

"""**BASELINE MODEL**

***Single Hidden Layer Neural Network***

Build Baseline Model
"""

baseline_model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

baseline_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

baseline_model.summary()

"""Train Baseline Model"""

history_baseline = baseline_model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

"""Evaluate Baseline Model"""

baseline_loss, baseline_acc = baseline_model.evaluate(X_test, y_test)
print(f"Baseline Test Accuracy: {baseline_acc:.4f}")

"""**IMPROVED MODEL**

***Deeper Network with More Neurons***

Build Improved Model
"""

improved_model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(256, activation='relu'),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

improved_model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

improved_model.summary()

"""Train Improved Model"""

history_improved = improved_model.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

"""Evaluate Improved Model"""

improved_loss, improved_acc = improved_model.evaluate(X_test, y_test)
print(f"Improved Test Accuracy: {improved_acc:.4f}")

"""**Plot Training & Validation Curves**

Accuracy Plot
"""

plt.figure(figsize=(8, 4))
plt.plot(history_baseline.history['accuracy'], label='Baseline Train')
plt.plot(history_baseline.history['val_accuracy'], label='Baseline Val')
plt.plot(history_improved.history['accuracy'], label='Improved Train')
plt.plot(history_improved.history['val_accuracy'], label='Improved Val')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Training and Validation Accuracy")
plt.legend()
plt.show()

"""Loss Plot"""

plt.figure(figsize=(8, 4))
plt.plot(history_baseline.history['loss'], label='Baseline Train')
plt.plot(history_baseline.history['val_loss'], label='Baseline Val')
plt.plot(history_improved.history['loss'], label='Improved Train')
plt.plot(history_improved.history['val_loss'], label='Improved Val')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.show()

"""**Model Comparison and Improvements**

***Baseline Model Architecture***

1.Consists of a single hidden layer with *128 neurons*.

2.Uses the *ReLU activation function*.

3.Learns basic *pixel-level features* such as edges and simple shapes.

4.Provides a reasonable baseline classification performance.

***Improved Model Architecture***

1.Introduces an *additional hidden layer with increased neurons.

2.Enables learning of *more complex and hierarchical feature representations.*

3.Better captures subtle differences between visually similar fashion items.

4.Uses a lower learning rate to ensure stable and smoother training.

***Performance Comparison***

1.Training and validation *curves show faster* and *more stable convergence* for the *improved model.*

2.The improved model achieves higher validation and test accuracy than the baseline model.

3.Loss values decrease more consistently, indicating better optimization.

***Key Takeaway***

Deeper architectures combined with appropriate hyperparameter tuning significantly enhance image classification performance.

This experiment demonstrates the importance of model depth and learning rate selection in neural network design.
"""